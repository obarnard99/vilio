{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-sperm",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import inspect\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir) \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "if args.tsv:\n",
    "    from features.vilio.fts_tsv.hm_data_tsv import HMTorchDataset, HMEvaluator, HMDataset\n",
    "else:\n",
    "    from features.vilio.fts_lmdb.hm_data import HMTorchDataset, HMEvaluator, HMDataset\n",
    "\n",
    "from src.vilio.transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "from utils.pandas_scripts import clean_data\n",
    "\n",
    "from models.U import ModelU\n",
    "from models.X import ModelX\n",
    "from models.V import ModelV\n",
    "from models.D import ModelD\n",
    "from models.O import ModelO\n",
    "\n",
    "# Two different SWA Methods - https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/\n",
    "if args.swa:\n",
    "    from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "\n",
    "if args.contrib:\n",
    "    from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HM:\n",
    "    def __init__(self):\n",
    "\n",
    "        if args.train is not None:\n",
    "            self.train_tuple = get_tuple(\n",
    "                args.train, bs=args.batch_size, shuffle=True, drop_last=False\n",
    "            )\n",
    "\n",
    "        if args.valid is not None:\n",
    "            valid_bsize = 2048 if args.multiGPU else 50\n",
    "            self.valid_tuple = get_tuple(\n",
    "                args.valid, bs=valid_bsize,\n",
    "                shuffle=False, drop_last=False\n",
    "            )\n",
    "        else:\n",
    "            self.valid_tuple = None\n",
    "\n",
    "        # Select Model, X is default\n",
    "        if args.model == \"X\":\n",
    "            self.model = ModelX(args)\n",
    "        elif args.model == \"V\":\n",
    "            self.model = ModelV(args)\n",
    "        elif args.model == \"U\":\n",
    "            self.model = ModelU(args)\n",
    "        elif args.model == \"D\":\n",
    "            self.model = ModelD(args)\n",
    "        elif args.model == 'O':\n",
    "            self.model = ModelO(args)\n",
    "        else:\n",
    "            print(args.model, \" is not implemented.\")\n",
    "\n",
    "        # Load pre-trained weights from paths\n",
    "        if args.loadpre is not None:\n",
    "            self.model.load(args.loadpre)\n",
    "\n",
    "        self.model = self.model.cuda(0)\n",
    "\n",
    "        # Losses and optimizer\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        self.nllloss = nn.NLLLoss()\n",
    "\n",
    "        def is_backbone(n):\n",
    "            if \"encoder\" in n:\n",
    "                return True\n",
    "            elif \"embeddings\" in n:\n",
    "                return True\n",
    "            elif \"pooler\" in n:\n",
    "                return True\n",
    "            print(\"F: \", n)\n",
    "            return False\n",
    "\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "        params = list(self.model.named_parameters())\n",
    "        if args.reg:\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\"params\": [p for n, p in params if is_backbone(n)], \"lr\": args.lr},\n",
    "                {\"params\": [p for n, p in params if not is_backbone(n)], \"lr\": args.lr * 500},\n",
    "            ]\n",
    "\n",
    "            for n, p in self.model.named_parameters():\n",
    "                print(n)\n",
    "\n",
    "            self.optim = AdamW(optimizer_grouped_parameters, lr=args.lr)\n",
    "        else:\n",
    "            optimizer_grouped_parameters = [\n",
    "                {'params': [p for n, p in params if not any(nd in n for nd in no_decay)], 'weight_decay': args.wd},\n",
    "                {'params': [p for n, p in params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "            ]\n",
    "\n",
    "            self.optim = AdamW(optimizer_grouped_parameters, lr=args.lr)\n",
    "\n",
    "        # SWA Method:\n",
    "        if args.contrib:\n",
    "            self.optim = SWA(self.optim, swa_start=self.t_total * 0.75, swa_freq=5, swa_lr=args.lr)\n",
    "\n",
    "        if args.swa:\n",
    "            self.swa_model = AveragedModel(self.model)\n",
    "            self.swa_start = self.t_total * 0.75\n",
    "            self.swa_scheduler = SWALR(self.optim, swa_lr=args.lr)\n",
    "\n",
    "    def predict(self, eval_tuple: DataTuple, dump=None, out_csv=True):\n",
    "\n",
    "        dset, loader, evaluator = eval_tuple\n",
    "        id2ans = {}\n",
    "        id2prob = {}\n",
    "\n",
    "        for i, datum_tuple in enumerate(loader):\n",
    "\n",
    "            ids, feats, boxes, sent = datum_tuple[:4]\n",
    "\n",
    "            self.model.eval()\n",
    "\n",
    "            if args.swa:\n",
    "                self.swa_model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                feats, boxes = feats.cuda(), boxes.cuda()\n",
    "                logit = self.model(sent, (feats, boxes))\n",
    "\n",
    "                # Note: LogSoftmax does not change order, hence there should be nothing wrong with taking it as our\n",
    "                # prediction\n",
    "                logit = self.logsoftmax(logit)\n",
    "                score = logit[:, 1]\n",
    "\n",
    "                if args.swa:\n",
    "                    logit = self.swa_model(sent, (feats, boxes))\n",
    "                    logit = self.logsoftmax(logit)\n",
    "\n",
    "                _, predict = logit.max(1)\n",
    "\n",
    "                for qid, l in zip(ids, predict.cpu().numpy()):\n",
    "                    id2ans[qid] = l\n",
    "\n",
    "                # Getting probas for Roc Auc\n",
    "                for qid, l in zip(ids, score.cpu().numpy()):\n",
    "                    id2prob[qid] = l\n",
    "\n",
    "        return id2ans, id2prob\n",
    "\n",
    "    def evaluate(self, eval_tuple: DataTuple, dump=None):\n",
    "        \"\"\"Evaluate all data in data_tuple.\"\"\"\n",
    "        id2ans, id2prob = self.predict(eval_tuple, dump=dump)\n",
    "\n",
    "        acc = eval_tuple.evaluator.evaluate(id2ans)\n",
    "        roc_auc = eval_tuple.evaluator.roc_auc(id2prob)\n",
    "\n",
    "        return acc, roc_auc\n",
    "\n",
    "    def load(self, path):\n",
    "        print(\"Load model from %s\" % path)\n",
    "\n",
    "        state_dict = torch.load(\"%s\" % path)\n",
    "        new_state_dict = {}\n",
    "        for key, value in state_dict.items():\n",
    "            # N_averaged is a key in SWA models we cannot load, so we skip it\n",
    "            if key.startswith(\"n_averaged\"):\n",
    "                print(\"n_averaged:\", value)\n",
    "                continue\n",
    "            # SWA Models will start with module\n",
    "            if key.startswith(\"module.\"):\n",
    "                new_state_dict[key[len(\"module.\"):]] = value\n",
    "            else:\n",
    "                new_state_dict[key] = value\n",
    "        state_dict = new_state_dict\n",
    "        self.model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataTuple = collections.namedtuple(\"DataTuple\", 'dataset loader evaluator')\n",
    "\n",
    "def get_tuple(splits: str, bs: int, shuffle=False, drop_last=False) -> DataTuple:\n",
    "    dset = HMDataset(splits)\n",
    "    tset = HMTorchDataset(splits, feature_path=args.features)\n",
    "    evaluator = HMEvaluator(tset)\n",
    "    data_loader = DataLoader(\n",
    "        tset, batch_size=bs,\n",
    "        shuffle=shuffle, num_workers=args.num_workers,\n",
    "        drop_last=drop_last, pin_memory=True\n",
    "    )\n",
    "\n",
    "    return DataTuple(dataset=dset, loader=data_loader, evaluator=evaluator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
